{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e65fa4fd",
   "metadata": {
    "id": "e65fa4fd"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from pickle import dump\n",
    "from unicodedata import normalize\n",
    "from numpy import array\n",
    "import string\n",
    "import re\n",
    "from numpy import array, argmax, random, take\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, RepeatVector\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from keras import optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "from pickle import load\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.layers import TimeDistributed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "m034xz0yesRZ",
   "metadata": {
    "id": "m034xz0yesRZ"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Model\n",
    "from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy, huber_loss, categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ieUwiVWajC7w",
   "metadata": {
    "id": "ieUwiVWajC7w"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    Load Dataset from File\n",
    "    \"\"\"\n",
    "    input_file = os.path.join(path)\n",
    "    with open(input_file, \"r\",encoding=\"utf8\") as f:\n",
    "        data = f.read().splitlines()\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "817a3d30",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "817a3d30",
    "outputId": "a91fdd76-bb5b-4bc0-b105-6f0f3236b48d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded\n"
     ]
    }
   ],
   "source": [
    "english_sentences = load_data('tal_eng.txt')\n",
    "tamil_sentences = load_data('tal_tal.txt')\n",
    "print('Dataset Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "QUJWoZerkVM-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QUJWoZerkVM-",
    "outputId": "2d84801b-4446-4d82-ac86-d53b75b45272"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(english_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "064ef733",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "064ef733",
    "outputId": "1582fe4c-4cfb-49c4-a06c-8726289ea579"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small_vocab_en Line 1:  joker\n",
      "small_vocab_fr Line 1:  கோமாளித்தனம்\n",
      "small_vocab_en Line 2:  Related Pages\n",
      "small_vocab_fr Line 2:  தொடர்பான பக்கங்கள்\n"
     ]
    }
   ],
   "source": [
    "for sample_i in range(2):\n",
    "    print('small_vocab_en Line {}:  {}'.format(sample_i + 1, english_sentences[sample_i]))\n",
    "    print('small_vocab_fr Line {}:  {}'.format(sample_i + 1, tamil_sentences[sample_i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "807fe3c4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "807fe3c4",
    "outputId": "59032ae0-2e48-4d2f-cbfa-2f7d372ed7f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12135 English words.\n",
      "4649 unique English words.\n",
      "10 Most common words in the English dataset:\n",
      "\"the\" \"in\" \"of\" \"and\" \"to\" \"is\" \"was\" \"In\" \"a\" \"people\"\n",
      "\n",
      "11335 tamil words.\n",
      "5886 unique tamil words.\n",
      "10 Most common words in the tamil dataset:\n",
      "\"-\" \"பேர்\" \"சோவியத்\" \"போர்\" \"ஒரு\" \"பல\" \"நாடுகள்\" \"நேச\" \"மே\" \"ஜூன்\"\n"
     ]
    }
   ],
   "source": [
    "english_words_counter = collections.Counter([word for sentence in english_sentences for word in sentence.split()])\n",
    "tamil_words_counter = collections.Counter([word for sentence in tamil_sentences for word in sentence.split()])\n",
    "print('{} English words.'.format(len([word for sentence in english_sentences for word in sentence.split()])))\n",
    "print('{} unique English words.'.format(len(english_words_counter)))\n",
    "print('10 Most common words in the English dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*english_words_counter.most_common(10)))[0]) + '\"')\n",
    "print()\n",
    "print('{} tamil words.'.format(len([word for sentence in tamil_sentences for word in sentence.split()])))\n",
    "print('{} unique tamil words.'.format(len(tamil_words_counter)))\n",
    "print('10 Most common words in the tamil dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*tamil_words_counter.most_common(10)))[0]) + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81799209",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "81799209",
    "outputId": "11a1cbaf-1f61-4e39-8f12-38559cd03c36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 1, 'quick': 2, 'a': 3, 'brown': 4, 'fox': 5, 'jumps': 6, 'over': 7, 'lazy': 8, 'dog': 9, 'by': 10, 'jove': 11, 'my': 12, 'study': 13, 'of': 14, 'lexicography': 15, 'won': 16, 'prize': 17, 'this': 18, 'is': 19, 'short': 20, 'sentence': 21}\n",
      "\n",
      "Sequence 1 in x\n",
      "  Input:  The quick brown fox jumps over the lazy dog .\n",
      "  Output: [1, 2, 4, 5, 6, 7, 1, 8, 9]\n",
      "Sequence 2 in x\n",
      "  Input:  By Jove , my quick study of lexicography won a prize .\n",
      "  Output: [10, 11, 12, 2, 13, 14, 15, 16, 3, 17]\n",
      "Sequence 3 in x\n",
      "  Input:  This is a short sentence .\n",
      "  Output: [18, 19, 3, 20, 21]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(x):\n",
    "    x_tk = Tokenizer(char_level = False)\n",
    "    x_tk.fit_on_texts(x)\n",
    "    return x_tk.texts_to_sequences(x), x_tk\n",
    "text_sentences = [\n",
    "    'The quick brown fox jumps over the lazy dog .',\n",
    "    'By Jove , my quick study of lexicography won a prize .',\n",
    "    'This is a short sentence .']\n",
    "text_tokenized, text_tokenizer = tokenize(text_sentences)\n",
    "print(text_tokenizer.word_index)\n",
    "print()\n",
    "for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(sent))\n",
    "    print('  Output: {}'.format(token_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0a61a93",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c0a61a93",
    "outputId": "28c40aa1-9ea9-4e01-910b-52afed4253fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1 in x\n",
      "  Input:  [1 2 4 5 6 7 1 8 9]\n",
      "  Output: [1 2 4 5 6 7 1 8 9 0]\n",
      "Sequence 2 in x\n",
      "  Input:  [10 11 12  2 13 14 15 16  3 17]\n",
      "  Output: [10 11 12  2 13 14 15 16  3 17]\n",
      "Sequence 3 in x\n",
      "  Input:  [18 19  3 20 21]\n",
      "  Output: [18 19  3 20 21  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "def pad(x, length=None):\n",
    "    if length is None:\n",
    "        length = max([len(sentence) for sentence in x])\n",
    "    return pad_sequences(x, maxlen = length, padding = 'post')\n",
    "# Pad Tokenized output\n",
    "test_pad = pad(text_tokenized)\n",
    "for sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(np.array(token_sent)))\n",
    "    print('  Output: {}'.format(pad_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c033886b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c033886b",
    "outputId": "eee95f2a-41eb-48b6-d9bf-fb8d0a5a798e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessed\n",
      "Max English sentence length: 82\n",
      "Max tamil sentence length: 59\n",
      "English vocabulary size: 3375\n",
      "Tamil vocabulary size: 5650\n"
     ]
    }
   ],
   "source": [
    "def preprocess(x, y):\n",
    "    preprocess_x, x_tk = tokenize(x)\n",
    "    preprocess_y, y_tk = tokenize(y)\n",
    "    preprocess_x = pad(preprocess_x)\n",
    "    preprocess_y = pad(preprocess_y)\n",
    "# Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
    "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
    "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
    "preproc_english_sentences, preproc_tamil_sentences, english_tokenizer, tamil_tokenizer =\\\n",
    "    preprocess(english_sentences, tamil_sentences)\n",
    "    \n",
    "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
    "max_tamil_sequence_length = preproc_tamil_sentences.shape[1]\n",
    "english_vocab_size = len(english_tokenizer.word_index)\n",
    "tamil_vocab_size = len(tamil_tokenizer.word_index)\n",
    "print('Data Preprocessed')\n",
    "print(\"Max English sentence length:\", max_english_sequence_length)\n",
    "print(\"Max tamil sentence length:\", max_tamil_sequence_length)\n",
    "print(\"English vocabulary size:\", english_vocab_size)\n",
    "print(\"Tamil vocabulary size:\", tamil_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9271a53",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b9271a53",
    "outputId": "5c0957b0-8748-42f1-a7e6-d61a44858a91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`logits_to_text` function loaded.\n"
     ]
    }
   ],
   "source": [
    "def logits_to_text(logits, tokenizer):\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<PAD>'\n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
    "print('`logits_to_text` function loaded.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5658710",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "f5658710",
    "outputId": "2143658f-ed86-49f6-d06f-a0d5b6bcb55f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "9/9 [==============================] - 11s 1s/step - loss: 206.5008 - accuracy: 0.8300 - val_loss: 337.0925 - val_accuracy: 0.8266\n",
      "Epoch 2/10\n",
      "9/9 [==============================] - 12s 1s/step - loss: 206.5008 - accuracy: 0.8300 - val_loss: 337.0925 - val_accuracy: 0.8266\n",
      "Epoch 3/10\n",
      "9/9 [==============================] - 12s 1s/step - loss: 206.5008 - accuracy: 0.8300 - val_loss: 337.0925 - val_accuracy: 0.8266\n",
      "Epoch 4/10\n",
      "9/9 [==============================] - 13s 1s/step - loss: 206.5008 - accuracy: 0.8300 - val_loss: 337.0925 - val_accuracy: 0.8266\n",
      "Epoch 5/10\n",
      "9/9 [==============================] - 15s 2s/step - loss: 206.5008 - accuracy: 0.8300 - val_loss: 337.0925 - val_accuracy: 0.8266\n",
      "Epoch 6/10\n",
      "9/9 [==============================] - 14s 2s/step - loss: 206.5008 - accuracy: 0.8300 - val_loss: 337.0925 - val_accuracy: 0.8266\n",
      "Epoch 7/10\n",
      "9/9 [==============================] - 11s 1s/step - loss: 206.5008 - accuracy: 0.8300 - val_loss: 337.0925 - val_accuracy: 0.8266\n",
      "Epoch 8/10\n",
      "9/9 [==============================] - 10s 1s/step - loss: 206.5008 - accuracy: 0.8300 - val_loss: 337.0925 - val_accuracy: 0.8266\n",
      "Epoch 9/10\n",
      "9/9 [==============================] - 10s 1s/step - loss: 206.5007 - accuracy: 0.8300 - val_loss: 337.0925 - val_accuracy: 0.8266\n",
      "Epoch 10/10\n",
      "9/9 [==============================] - 9s 972ms/step - loss: 206.5008 - accuracy: 0.8300 - val_loss: 337.0925 - val_accuracy: 0.8266\n",
      "1/1 [==============================] - 0s 419ms/step\n",
      "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "def simple_model(input_shape, output_sequence_length, english_vocab_size, tamil_vocab_size):\n",
    "    learning_rate = 1e-3\n",
    "    input_seq = Input(input_shape[1:])\n",
    "    rnn = GRU(64, return_sequences = True)(input_seq)\n",
    "    logits = TimeDistributed(Dense(4720))(rnn)\n",
    "    model = Model(input_seq, Activation('softmax')(logits))\n",
    "    model.compile(loss = huber_loss, \n",
    "                 optimizer = Adam(learning_rate), \n",
    "                 metrics = ['accuracy'])\n",
    "    \n",
    "    return model\n",
    "tmp_x = pad(preproc_english_sentences, max_tamil_sequence_length)\n",
    "tmp_x = tmp_x.reshape((-1, preproc_tamil_sentences.shape[-2], 1))\n",
    "# Train the neural network\n",
    "simple_rnn_model = simple_model(\n",
    "    tmp_x.shape,\n",
    "    max_tamil_sequence_length,\n",
    "    english_vocab_size,\n",
    "    tamil_vocab_size)\n",
    "simple_rnn_model.fit(tmp_x, preproc_tamil_sentences, batch_size=128, epochs=10, validation_split=0.2)\n",
    "# Print prediction(s)\n",
    "print(logits_to_text(simple_rnn_model.predict(tmp_x[:1])[0], tamil_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60c1f12a",
   "metadata": {
    "id": "60c1f12a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2/2 [==============================] - 67s 2s/step - loss: 206.5008 - accuracy: 0.0000e+00 - val_loss: 337.0925 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 54s 1s/step - loss: 206.5008 - accuracy: 0.0000e+00 - val_loss: 337.0925 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 58s 1s/step - loss: 206.5008 - accuracy: 0.0000e+00 - val_loss: 337.0925 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 77s 2s/step - loss: 206.5008 - accuracy: 0.0000e+00 - val_loss: 337.0925 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 54s 1s/step - loss: 206.5008 - accuracy: 0.0000e+00 - val_loss: 337.0925 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 73s 2s/step - loss: 206.5008 - accuracy: 0.0000e+00 - val_loss: 337.0925 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 62s 2s/step - loss: 206.5008 - accuracy: 0.0000e+00 - val_loss: 337.0925 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 59s 2s/step - loss: 206.5008 - accuracy: 0.0000e+00 - val_loss: 337.0925 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 71s 2s/step - loss: 206.5008 - accuracy: 0.0000e+00 - val_loss: 337.0925 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 79s 2s/step - loss: 206.5008 - accuracy: 0.0000e+00 - val_loss: 337.0925 - val_accuracy: 0.0000e+00\n",
      "1/1 [==============================] - 1s 678ms/step\n",
      "உள்ளுணர்வும் உள்ளுணர்வும் உள்ளுணர்வும் உள்ளுணர்வும் உள்ளுணர்வும் உள்ளுணர்வும் உள்ளுணர்வும் உள்ளுணர்வும் உள்ளுணர்வும் உள்ளுணர்வும் உள்ளுணர்வும் உள்ளுணர்வும் உள்ளுணர்வும் உள்ளுணர்வும் உள்ளுணர்வும் உள்ளுணர்வும் உள்ளுணர்வும் உள்ளுணர்வும் உள்ளுணர்வும் உள்ளுணர்வும் உள்ளுணர்வும் உள்ளுணர்வும் உள்ளுணர்வும் உள்ளுணர்வும் உள்ளுணர்வும் உள்ளுணர்வும் உள்ளுணர்வும் உள்ளுணர்வும் உள்ளுணர்வும் உள்ளுணர்வும் உள்ளுணர்வும் உள்ளுணர்வும் உள்ளுணர்வும் உள்ளுணர்வும் உள்ளுணர்வும் உள்ளுணர்வும் உள்ளுணர்வும் உள்ளுணர்வும் உள்ளுணர்வும் உள்ளுணர்வும் உள்ளுணர்வும் உள்ளுணர்வும் உள்ளுணர்வும் உள்ளுணர்வும் உள்ளுணர்வும் உள்ளுணர்வும் உள்ளுணர்வும் உள்ளுணர்வும் உள்ளுணர்வும் உள்ளுணர்வும் உள்ளுணர்வும் உள்ளுணர்வும் உள்ளுணர்வும் உள்ளுணர்வும் உள்ளுணர்வும் உள்ளுணர்வும் உள்ளுணர்வும் உள்ளுணர்வும் உள்ளுணர்வும்\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "def embed_model(input_shape, output_sequence_length, english_vocab_size, tamil_vocab_size):\n",
    "    learning_rate = 1e-3\n",
    "    rnn = GRU(64, return_sequences=True, activation=\"tanh\")\n",
    "    \n",
    "    embedding = Embedding(tamil_vocab_size, 64, input_length=input_shape[1]) \n",
    "    logits = TimeDistributed(Dense(tamil_vocab_size, activation=\"softmax\"))\n",
    "    \n",
    "    model = Sequential()\n",
    "    #em can only be used in first layer --> Keras Documentation\n",
    "    model.add(embedding)\n",
    "    model.add(rnn)\n",
    "    model.add(logits)\n",
    "    model.compile(loss=huber_loss,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "tmp_x = pad(preproc_english_sentences, max_tamil_sequence_length)\n",
    "tmp_x = tmp_x.reshape((-1, preproc_tamil_sentences.shape[-2]))\n",
    "embeded_model = embed_model(\n",
    "    tmp_x.shape,\n",
    "    max_tamil_sequence_length,\n",
    "    english_vocab_size,\n",
    "    tamil_vocab_size)\n",
    "embeded_model.fit(tmp_x, preproc_tamil_sentences, batch_size=1024, epochs=10, validation_split=0.2)\n",
    "print(logits_to_text(embeded_model.predict(tmp_x[:1])[0], tamil_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a414e14",
   "metadata": {
    "id": "4a414e14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2/2 [==============================] - 41s 2s/step - loss: 206.5008 - accuracy: 0.8304 - val_loss: 337.0925 - val_accuracy: 0.8268\n",
      "Epoch 2/20\n",
      "2/2 [==============================] - 29s 2s/step - loss: 206.5008 - accuracy: 0.8305 - val_loss: 337.0925 - val_accuracy: 0.8268\n",
      "Epoch 3/20\n",
      "2/2 [==============================] - 28s 2s/step - loss: 206.5008 - accuracy: 0.8310 - val_loss: 337.0925 - val_accuracy: 0.8268\n",
      "Epoch 4/20\n",
      "2/2 [==============================] - 33s 2s/step - loss: 206.5008 - accuracy: 0.8307 - val_loss: 337.0925 - val_accuracy: 0.8268\n",
      "Epoch 5/20\n",
      "2/2 [==============================] - 33s 2s/step - loss: 206.5008 - accuracy: 0.8306 - val_loss: 337.0925 - val_accuracy: 0.8268\n",
      "Epoch 6/20\n",
      "2/2 [==============================] - 35s 2s/step - loss: 206.5008 - accuracy: 0.8306 - val_loss: 337.0925 - val_accuracy: 0.8268\n",
      "Epoch 7/20\n",
      "2/2 [==============================] - 27s 2s/step - loss: 206.5008 - accuracy: 0.8308 - val_loss: 337.0925 - val_accuracy: 0.8268\n",
      "Epoch 8/20\n",
      "2/2 [==============================] - 30s 2s/step - loss: 206.5008 - accuracy: 0.8305 - val_loss: 337.0925 - val_accuracy: 0.8268\n",
      "Epoch 9/20\n",
      "2/2 [==============================] - 31s 2s/step - loss: 206.5008 - accuracy: 0.8309 - val_loss: 337.0925 - val_accuracy: 0.8268\n",
      "Epoch 10/20\n",
      "2/2 [==============================] - 29s 2s/step - loss: 206.5008 - accuracy: 0.8310 - val_loss: 337.0925 - val_accuracy: 0.8268\n",
      "Epoch 11/20\n",
      "2/2 [==============================] - 30s 2s/step - loss: 206.5008 - accuracy: 0.8305 - val_loss: 337.0925 - val_accuracy: 0.8268\n",
      "Epoch 12/20\n",
      "2/2 [==============================] - 30s 2s/step - loss: 206.5008 - accuracy: 0.8309 - val_loss: 337.0925 - val_accuracy: 0.8268\n",
      "Epoch 13/20\n",
      "2/2 [==============================] - 31s 2s/step - loss: 206.5008 - accuracy: 0.8307 - val_loss: 337.0925 - val_accuracy: 0.8268\n",
      "Epoch 14/20\n",
      "2/2 [==============================] - 32s 2s/step - loss: 206.5008 - accuracy: 0.8310 - val_loss: 337.0925 - val_accuracy: 0.8268\n",
      "Epoch 15/20\n",
      "2/2 [==============================] - 31s 2s/step - loss: 206.5007 - accuracy: 0.8303 - val_loss: 337.0925 - val_accuracy: 0.8268\n",
      "Epoch 16/20\n",
      "2/2 [==============================] - 33s 2s/step - loss: 206.5008 - accuracy: 0.8311 - val_loss: 337.0925 - val_accuracy: 0.8268\n",
      "Epoch 17/20\n",
      "2/2 [==============================] - 37s 2s/step - loss: 206.5008 - accuracy: 0.8302 - val_loss: 337.0925 - val_accuracy: 0.8268\n",
      "Epoch 18/20\n",
      "2/2 [==============================] - 29s 2s/step - loss: 206.5008 - accuracy: 0.8313 - val_loss: 337.0925 - val_accuracy: 0.8268\n",
      "Epoch 19/20\n",
      "2/2 [==============================] - 35s 2s/step - loss: 206.5008 - accuracy: 0.8305 - val_loss: 337.0925 - val_accuracy: 0.8268\n",
      "Epoch 20/20\n",
      "2/2 [==============================] - 33s 2s/step - loss: 206.5008 - accuracy: 0.8302 - val_loss: 337.0925 - val_accuracy: 0.8268\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "def bd_model(input_shape, output_sequence_length, english_vocab_size, tamil_vocab_size):\n",
    "   \n",
    "    learning_rate = 1e-3\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(GRU(128, return_sequences = True, dropout = 0.1), \n",
    "                           input_shape = input_shape[1:]))\n",
    "    model.add(TimeDistributed(Dense(tamil_vocab_size, activation = 'softmax')))\n",
    "    model.compile(loss = huber_loss, \n",
    "                 optimizer = Adam(learning_rate), \n",
    "                 metrics = ['accuracy'])\n",
    "    return model\n",
    "tmp_x = pad(preproc_english_sentences, preproc_tamil_sentences.shape[1])\n",
    "tmp_x = tmp_x.reshape((-1, preproc_tamil_sentences.shape[-2], 1))\n",
    "bidi_model = bd_model(\n",
    "    tmp_x.shape,\n",
    "    preproc_tamil_sentences.shape[1],\n",
    "    len(english_tokenizer.word_index)+1,\n",
    "    len(tamil_tokenizer.word_index)+1)\n",
    "bidi_model.fit(tmp_x, preproc_tamil_sentences, batch_size=1024, epochs=20, validation_split=0.2)\n",
    "# Print prediction(s)\n",
    "print(logits_to_text(bidi_model.predict(tmp_x[:1])[0], tamil_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d1cc051",
   "metadata": {
    "id": "2d1cc051"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2/2 [==============================] - 43s 3s/step - loss: 206.5008 - accuracy: 0.8519 - val_loss: 337.0925 - val_accuracy: 0.8451\n",
      "Epoch 2/20\n",
      "2/2 [==============================] - 36s 2s/step - loss: 206.5008 - accuracy: 0.8519 - val_loss: 337.0925 - val_accuracy: 0.8451\n",
      "Epoch 3/20\n",
      "2/2 [==============================] - 36s 2s/step - loss: 206.5008 - accuracy: 0.8519 - val_loss: 337.0925 - val_accuracy: 0.8451\n",
      "Epoch 4/20\n",
      "2/2 [==============================] - 33s 2s/step - loss: 206.5008 - accuracy: 0.8519 - val_loss: 337.0925 - val_accuracy: 0.8451\n",
      "Epoch 5/20\n",
      "2/2 [==============================] - 41s 2s/step - loss: 206.5008 - accuracy: 0.8519 - val_loss: 337.0925 - val_accuracy: 0.8451\n",
      "Epoch 6/20\n",
      "2/2 [==============================] - 39s 2s/step - loss: 206.5008 - accuracy: 0.8519 - val_loss: 337.0925 - val_accuracy: 0.8451\n",
      "Epoch 7/20\n",
      "2/2 [==============================] - 36s 2s/step - loss: 206.5008 - accuracy: 0.8519 - val_loss: 337.0925 - val_accuracy: 0.8451\n",
      "Epoch 8/20\n",
      "2/2 [==============================] - 50s 3s/step - loss: 206.5008 - accuracy: 0.8519 - val_loss: 337.0925 - val_accuracy: 0.8451\n",
      "Epoch 9/20\n",
      "2/2 [==============================] - 1532s 2s/step - loss: 206.5008 - accuracy: 0.8519 - val_loss: 337.0925 - val_accuracy: 0.8451\n",
      "Epoch 10/20\n",
      "2/2 [==============================] - 54s 2s/step - loss: 206.5008 - accuracy: 0.8519 - val_loss: 337.0925 - val_accuracy: 0.8451\n",
      "Epoch 11/20\n",
      "2/2 [==============================] - 40s 2s/step - loss: 206.5008 - accuracy: 0.8519 - val_loss: 337.0925 - val_accuracy: 0.8451\n",
      "Epoch 12/20\n",
      "2/2 [==============================] - 36s 2s/step - loss: 206.5008 - accuracy: 0.8519 - val_loss: 337.0925 - val_accuracy: 0.8451\n",
      "Epoch 13/20\n",
      "2/2 [==============================] - 32s 2s/step - loss: 206.5008 - accuracy: 0.8519 - val_loss: 337.0925 - val_accuracy: 0.8451\n",
      "Epoch 14/20\n",
      "2/2 [==============================] - 34s 2s/step - loss: 206.5008 - accuracy: 0.8519 - val_loss: 337.0925 - val_accuracy: 0.8451\n",
      "Epoch 15/20\n",
      "2/2 [==============================] - 34s 2s/step - loss: 206.5008 - accuracy: 0.8519 - val_loss: 337.0925 - val_accuracy: 0.8451\n",
      "Epoch 16/20\n",
      "2/2 [==============================] - 33s 2s/step - loss: 206.5008 - accuracy: 0.8519 - val_loss: 337.0925 - val_accuracy: 0.8451\n",
      "Epoch 17/20\n",
      "2/2 [==============================] - 34s 2s/step - loss: 206.5008 - accuracy: 0.8519 - val_loss: 337.0925 - val_accuracy: 0.8451\n",
      "Epoch 18/20\n",
      "2/2 [==============================] - 32s 2s/step - loss: 206.5008 - accuracy: 0.8519 - val_loss: 337.0925 - val_accuracy: 0.8451\n",
      "Epoch 19/20\n",
      "2/2 [==============================] - 29s 2s/step - loss: 206.5008 - accuracy: 0.8519 - val_loss: 337.0925 - val_accuracy: 0.8451\n",
      "Epoch 20/20\n",
      "2/2 [==============================] - 36s 2s/step - loss: 206.5008 - accuracy: 0.8519 - val_loss: 337.0925 - val_accuracy: 0.8451\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "def encdec_model(input_shape, output_sequence_length, english_vocab_size, tamil_vocab_size):\n",
    "  \n",
    "    learning_rate = 1e-3\n",
    "    model = Sequential()\n",
    "    model.add(GRU(128, input_shape = input_shape[1:], return_sequences = False))\n",
    "    model.add(RepeatVector(output_sequence_length))\n",
    "    model.add(GRU(128, return_sequences = True))\n",
    "    model.add(TimeDistributed(Dense(tamil_vocab_size, activation = 'softmax')))\n",
    "    \n",
    "    model.compile(loss = huber_loss, \n",
    "                 optimizer = Adam(learning_rate), \n",
    "                 metrics = ['accuracy'])\n",
    "    return model\n",
    "tmp_x = pad(preproc_english_sentences)\n",
    "tmp_x = tmp_x.reshape((-1, preproc_english_sentences.shape[1], 1))\n",
    "encodeco_model = encdec_model(\n",
    "    tmp_x.shape,\n",
    "    preproc_tamil_sentences.shape[1],\n",
    "    len(english_tokenizer.word_index)+1,\n",
    "    len(tamil_tokenizer.word_index)+1)\n",
    "encodeco_model.fit(tmp_x, preproc_tamil_sentences, batch_size=1024, epochs=20, validation_split=0.2)\n",
    "print(logits_to_text(encodeco_model.predict(tmp_x[:1])[0], tamil_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d257dbd4",
   "metadata": {
    "id": "d257dbd4",
    "outputId": "53f81117-d58e-4d91-b64f-68d485e01781"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model Loaded\n"
     ]
    }
   ],
   "source": [
    "def model_final(input_shape, output_sequence_length, english_vocab_size, tamil_vocab_size):\n",
    "  \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=english_vocab_size,output_dim=128,input_length=input_shape[1]))\n",
    "    model.add(Bidirectional(GRU(256,return_sequences=False)))\n",
    "    model.add(RepeatVector(output_sequence_length))\n",
    "    model.add(Bidirectional(GRU(256,return_sequences=True)))\n",
    "    model.add(TimeDistributed(Dense(tamil_vocab_size,activation='softmax')))\n",
    "    learning_rate = 0.005\n",
    "    \n",
    "    model.compile(loss = sparse_categorical_crossentropy, \n",
    "                 optimizer = Adam(learning_rate), \n",
    "                 metrics = ['accuracy'])\n",
    "    \n",
    "    return model\n",
    "print('Final Model Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78bdf123",
   "metadata": {
    "id": "78bdf123"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/17\n",
      "2/2 [==============================] - 92s 5s/step - loss: 8.6338 - accuracy: 0.0073 - val_loss: 2.8550 - val_accuracy: 0.8492\n",
      "Epoch 2/17\n",
      "2/2 [==============================] - 141s 4s/step - loss: 2.8117 - accuracy: 0.8540 - val_loss: 2.0450 - val_accuracy: 0.8492\n",
      "Epoch 3/17\n",
      "2/2 [==============================] - 84s 3s/step - loss: 1.9537 - accuracy: 0.8540 - val_loss: 1.6771 - val_accuracy: 0.8492\n",
      "Epoch 4/17\n",
      "2/2 [==============================] - 92s 3s/step - loss: 1.5628 - accuracy: 0.8540 - val_loss: 1.6554 - val_accuracy: 0.8492\n",
      "Epoch 5/17\n",
      "2/2 [==============================] - 76s 3s/step - loss: 1.4952 - accuracy: 0.8540 - val_loss: 1.8522 - val_accuracy: 0.8492\n",
      "Epoch 6/17\n",
      "2/2 [==============================] - 70s 3s/step - loss: 1.6432 - accuracy: 0.8540 - val_loss: 1.7666 - val_accuracy: 0.8492\n",
      "Epoch 7/17\n",
      "2/2 [==============================] - 70s 3s/step - loss: 1.5170 - accuracy: 0.8540 - val_loss: 1.8358 - val_accuracy: 0.8492\n",
      "Epoch 8/17\n",
      "2/2 [==============================] - 70s 3s/step - loss: 1.5672 - accuracy: 0.8540 - val_loss: 1.7835 - val_accuracy: 0.8492\n",
      "Epoch 9/17\n",
      "2/2 [==============================] - 65s 3s/step - loss: 1.4927 - accuracy: 0.8540 - val_loss: 1.8358 - val_accuracy: 0.8492\n",
      "Epoch 10/17\n",
      "2/2 [==============================] - 67s 3s/step - loss: 1.5254 - accuracy: 0.8540 - val_loss: 1.8523 - val_accuracy: 0.8492\n",
      "Epoch 11/17\n",
      "2/2 [==============================] - 67s 3s/step - loss: 1.5304 - accuracy: 0.8540 - val_loss: 1.8543 - val_accuracy: 0.8492\n",
      "Epoch 12/17\n",
      "2/2 [==============================] - 70s 3s/step - loss: 1.5198 - accuracy: 0.8540 - val_loss: 1.8569 - val_accuracy: 0.8492\n",
      "Epoch 13/17\n",
      "2/2 [==============================] - 69s 3s/step - loss: 1.5170 - accuracy: 0.8540 - val_loss: 1.9069 - val_accuracy: 0.8492\n",
      "Epoch 14/17\n",
      "2/2 [==============================] - 65s 3s/step - loss: 1.5759 - accuracy: 0.8540 - val_loss: 1.8638 - val_accuracy: 0.8492\n",
      "Epoch 15/17\n",
      "2/2 [==============================] - 68s 3s/step - loss: 1.4833 - accuracy: 0.8540 - val_loss: 1.8968 - val_accuracy: 0.8492\n",
      "Epoch 16/17\n",
      "2/2 [==============================] - 80s 3s/step - loss: 1.4981 - accuracy: 0.8540 - val_loss: 1.8583 - val_accuracy: 0.8492\n",
      "Epoch 17/17\n",
      "2/2 [==============================] - 84s 3s/step - loss: 1.4641 - accuracy: 0.8540 - val_loss: 1.8696 - val_accuracy: 0.8492\n"
     ]
    }
   ],
   "source": [
    "def final_predictions(x, y, x_tk, y_tk):\n",
    "    global predictions\n",
    "    tmp_X = pad(preproc_english_sentences)\n",
    "    model = model_final(tmp_X.shape,\n",
    "                        preproc_tamil_sentences.shape[1],\n",
    "                        len(english_tokenizer.word_index)+1,\n",
    "                        len(tamil_tokenizer.word_index)+1)\n",
    "    \n",
    "    model.fit(tmp_X, preproc_tamil_sentences, batch_size = 1024, epochs = 17, validation_split = 0.2)\n",
    " \n",
    "    \n",
    "final_predictions(preproc_english_sentences, preproc_tamil_sentences, english_tokenizer, tamil_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97927a48",
   "metadata": {
    "id": "97927a48"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a67bc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
