{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e65fa4fd",
   "metadata": {
    "id": "e65fa4fd"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from pickle import dump\n",
    "from unicodedata import normalize\n",
    "from numpy import array\n",
    "import string\n",
    "import re\n",
    "from numpy import array, argmax, random, take\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, RepeatVector\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from keras import optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "from pickle import load\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.layers import TimeDistributed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "m034xz0yesRZ",
   "metadata": {
    "id": "m034xz0yesRZ"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Model\n",
    "from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy, huber_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ieUwiVWajC7w",
   "metadata": {
    "id": "ieUwiVWajC7w"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    Load Dataset from File\n",
    "    \"\"\"\n",
    "    input_file = os.path.join(path)\n",
    "    with open(input_file, \"r\",encoding=\"utf8\") as f:\n",
    "        data = f.read().splitlines()\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "817a3d30",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "817a3d30",
    "outputId": "a91fdd76-bb5b-4bc0-b105-6f0f3236b48d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded\n"
     ]
    }
   ],
   "source": [
    "english_sentences = load_data('tel_eng.txt')\n",
    "french_sentences = load_data('tel_tel.txt')\n",
    "print('Dataset Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "QUJWoZerkVM-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QUJWoZerkVM-",
    "outputId": "2d84801b-4446-4d82-ac86-d53b75b45272"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(english_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "064ef733",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "064ef733",
    "outputId": "1582fe4c-4cfb-49c4-a06c-8726289ea579"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small_vocab_en Line 1:  After Independence, the Indian government taken police action upon the Nizam's kingdom then they fromed these places as a Hyderabad state.\n",
      "small_vocab_fr Line 1:  స్వాతంత్ర్యం తరువాత భారత ప్రభుత్వం నిజాము సంస్థానంపై జరిపిన పోలీసు చర్య తరువాత ఈ ప్రాంతాలు హైదరాబాదు రాష్ట్రంగా ఏర్పడ్డాయి.\n",
      "small_vocab_en Line 2:  Michael Jones Jackson(Birth August 29 1958)(Death June 25 2009)a musical artist belonged to America.\n",
      "small_vocab_fr Line 2:  మైకల్ జోసెఫ్ జాక్సన్ (జననం ఆగష్టు 29 1958)(మరణం జూన్ 25 2009) అమెరికాకు చెందిన ఒక ప్రముఖ సంగీత కళాకారుడు.\n"
     ]
    }
   ],
   "source": [
    "for sample_i in range(2):\n",
    "    print('small_vocab_en Line {}:  {}'.format(sample_i + 1, english_sentences[sample_i]))\n",
    "    print('small_vocab_fr Line {}:  {}'.format(sample_i + 1, french_sentences[sample_i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "807fe3c4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "807fe3c4",
    "outputId": "59032ae0-2e48-4d2f-cbfa-2f7d372ed7f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10467 English words.\n",
      "4451 unique English words.\n",
      "10 Most common words in the English dataset:\n",
      "\"the\" \"of\" \"in\" \"is\" \"and\" \"to\" \"are\" \"as\" \"this\" \"was\"\n",
      "\n",
      "9290 French words.\n",
      "5115 unique French words.\n",
      "10 Most common words in the French dataset:\n",
      "\"ఈ\" \":\" \"-\" \"విశాఖపట్నం\" \"అమెరికా\" \"ఒక\" \"స్థాపన\" \"ఉన్న\" \"విశాఖ\" \"నుంచి\"\n"
     ]
    }
   ],
   "source": [
    "english_words_counter = collections.Counter([word for sentence in english_sentences for word in sentence.split()])\n",
    "french_words_counter = collections.Counter([word for sentence in french_sentences for word in sentence.split()])\n",
    "print('{} English words.'.format(len([word for sentence in english_sentences for word in sentence.split()])))\n",
    "print('{} unique English words.'.format(len(english_words_counter)))\n",
    "print('10 Most common words in the English dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*english_words_counter.most_common(10)))[0]) + '\"')\n",
    "print()\n",
    "print('{} French words.'.format(len([word for sentence in french_sentences for word in sentence.split()])))\n",
    "print('{} unique French words.'.format(len(french_words_counter)))\n",
    "print('10 Most common words in the French dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*french_words_counter.most_common(10)))[0]) + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81799209",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "81799209",
    "outputId": "11a1cbaf-1f61-4e39-8f12-38559cd03c36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 1, 'quick': 2, 'a': 3, 'brown': 4, 'fox': 5, 'jumps': 6, 'over': 7, 'lazy': 8, 'dog': 9, 'by': 10, 'jove': 11, 'my': 12, 'study': 13, 'of': 14, 'lexicography': 15, 'won': 16, 'prize': 17, 'this': 18, 'is': 19, 'short': 20, 'sentence': 21}\n",
      "\n",
      "Sequence 1 in x\n",
      "  Input:  The quick brown fox jumps over the lazy dog .\n",
      "  Output: [1, 2, 4, 5, 6, 7, 1, 8, 9]\n",
      "Sequence 2 in x\n",
      "  Input:  By Jove , my quick study of lexicography won a prize .\n",
      "  Output: [10, 11, 12, 2, 13, 14, 15, 16, 3, 17]\n",
      "Sequence 3 in x\n",
      "  Input:  This is a short sentence .\n",
      "  Output: [18, 19, 3, 20, 21]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(x):\n",
    "    x_tk = Tokenizer(char_level = False)\n",
    "    x_tk.fit_on_texts(x)\n",
    "    return x_tk.texts_to_sequences(x), x_tk\n",
    "text_sentences = [\n",
    "    'The quick brown fox jumps over the lazy dog .',\n",
    "    'By Jove , my quick study of lexicography won a prize .',\n",
    "    'This is a short sentence .']\n",
    "text_tokenized, text_tokenizer = tokenize(text_sentences)\n",
    "print(text_tokenizer.word_index)\n",
    "print()\n",
    "for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(sent))\n",
    "    print('  Output: {}'.format(token_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0a61a93",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c0a61a93",
    "outputId": "28c40aa1-9ea9-4e01-910b-52afed4253fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1 in x\n",
      "  Input:  [1 2 4 5 6 7 1 8 9]\n",
      "  Output: [1 2 4 5 6 7 1 8 9 0]\n",
      "Sequence 2 in x\n",
      "  Input:  [10 11 12  2 13 14 15 16  3 17]\n",
      "  Output: [10 11 12  2 13 14 15 16  3 17]\n",
      "Sequence 3 in x\n",
      "  Input:  [18 19  3 20 21]\n",
      "  Output: [18 19  3 20 21  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "def pad(x, length=None):\n",
    "    if length is None:\n",
    "        length = max([len(sentence) for sentence in x])\n",
    "    return pad_sequences(x, maxlen = length, padding = 'post')\n",
    "# Pad Tokenized output\n",
    "test_pad = pad(text_tokenized)\n",
    "for sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(np.array(token_sent)))\n",
    "    print('  Output: {}'.format(pad_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c033886b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c033886b",
    "outputId": "eee95f2a-41eb-48b6-d9bf-fb8d0a5a798e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessed\n",
      "Max English sentence length: 57\n",
      "Max French sentence length: 52\n",
      "English vocabulary size: 3391\n",
      "French vocabulary size: 4720\n"
     ]
    }
   ],
   "source": [
    "def preprocess(x, y):\n",
    "    preprocess_x, x_tk = tokenize(x)\n",
    "    preprocess_y, y_tk = tokenize(y)\n",
    "    preprocess_x = pad(preprocess_x)\n",
    "    preprocess_y = pad(preprocess_y)\n",
    "# Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
    "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
    "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
    "preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer =\\\n",
    "    preprocess(english_sentences, french_sentences)\n",
    "    \n",
    "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
    "max_french_sequence_length = preproc_french_sentences.shape[1]\n",
    "english_vocab_size = len(english_tokenizer.word_index)\n",
    "french_vocab_size = len(french_tokenizer.word_index)\n",
    "print('Data Preprocessed')\n",
    "print(\"Max English sentence length:\", max_english_sequence_length)\n",
    "print(\"Max French sentence length:\", max_french_sequence_length)\n",
    "print(\"English vocabulary size:\", english_vocab_size)\n",
    "print(\"French vocabulary size:\", french_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9271a53",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b9271a53",
    "outputId": "5c0957b0-8748-42f1-a7e6-d61a44858a91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`logits_to_text` function loaded.\n"
     ]
    }
   ],
   "source": [
    "def logits_to_text(logits, tokenizer):\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<PAD>'\n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
    "print('`logits_to_text` function loaded.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f5658710",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "f5658710",
    "outputId": "2143658f-ed86-49f6-d06f-a0d5b6bcb55f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "8/8 [==============================] - 11s 1s/step - loss: 177.3801 - accuracy: 0.4879 - val_loss: 254.0947 - val_accuracy: 0.5030\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 8s 1s/step - loss: 177.3801 - accuracy: 0.4875 - val_loss: 254.0947 - val_accuracy: 0.5025\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 8s 1s/step - loss: 177.3801 - accuracy: 0.4871 - val_loss: 254.0947 - val_accuracy: 0.5020\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 8s 1s/step - loss: 177.3801 - accuracy: 0.4862 - val_loss: 254.0947 - val_accuracy: 0.5009\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 8s 1000ms/step - loss: 177.3801 - accuracy: 0.4851 - val_loss: 254.0947 - val_accuracy: 0.4998\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 8s 1s/step - loss: 177.3801 - accuracy: 0.4840 - val_loss: 254.0947 - val_accuracy: 0.4989\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 9s 1s/step - loss: 177.3801 - accuracy: 0.4826 - val_loss: 254.0947 - val_accuracy: 0.4983\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 8s 1s/step - loss: 177.3801 - accuracy: 0.4817 - val_loss: 254.0947 - val_accuracy: 0.4976\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 8s 1s/step - loss: 177.3801 - accuracy: 0.4810 - val_loss: 254.0947 - val_accuracy: 0.4971\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 8s 1s/step - loss: 177.3801 - accuracy: 0.4804 - val_loss: 254.0947 - val_accuracy: 0.4968\n",
      "1/1 [==============================] - 0s 430ms/step\n",
      "షిప్ షిప్ షిప్ షిప్ నౌకా షిప్ షిప్ షిప్ జరిగిన కమ్యూనిటీ విద్యను విద్యను కొండపై నిర్మించనున్న సంస్థానంలో నిర్మించనున్న కొండపై కొండపై నిరుద్యోగ నిరుద్యోగ నిరుద్యోగ నిరుద్యోగ నిరుద్యోగ నిరుద్యోగ నిరుద్యోగ నిరుద్యోగ నిరుద్యోగ నిరుద్యోగ నిరుద్యోగ నిరుద్యోగ పురుషార్థము ఆఫ్రికన్‌ ఆఫ్రికన్‌ ఆఫ్రికన్‌ ఆఫ్రికన్‌ ఆఫ్రికన్‌ ఆఫ్రికన్‌ ఆఫ్రికన్‌ ఆఫ్రికన్‌ ఆఫ్రికన్‌ కారణంగా కారణంగా కారణంగా కారణంగా కారణంగా కారణంగా జల టౌన్ అధికారిక విశాఖపట్టణం నుంచి అమెరికా\n"
     ]
    }
   ],
   "source": [
    "def simple_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    learning_rate = 1e-3\n",
    "    input_seq = Input(input_shape[1:])\n",
    "    rnn = GRU(64, return_sequences = True)(input_seq)\n",
    "    logits = TimeDistributed(Dense(4720))(rnn)\n",
    "    model = Model(input_seq, Activation('softmax')(logits))\n",
    "    model.compile(loss = huber_loss, \n",
    "                 optimizer = Adam(learning_rate), \n",
    "                 metrics = ['accuracy'])\n",
    "    \n",
    "    return model\n",
    "tmp_x = pad(preproc_english_sentences, max_french_sequence_length)\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\n",
    "# Train the neural network\n",
    "simple_rnn_model = simple_model(\n",
    "    tmp_x.shape,\n",
    "    max_french_sequence_length,\n",
    "    english_vocab_size,\n",
    "    french_vocab_size)\n",
    "simple_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=128, epochs=10, validation_split=0.2)\n",
    "# Print prediction(s)\n",
    "print(logits_to_text(simple_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "60c1f12a",
   "metadata": {
    "id": "60c1f12a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 40s 40s/step - loss: 177.3801 - accuracy: 5.7121e-05 - val_loss: 254.0947 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 21s 21s/step - loss: 177.3801 - accuracy: 5.7121e-05 - val_loss: 254.0947 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 22s 22s/step - loss: 177.3801 - accuracy: 5.7121e-05 - val_loss: 254.0947 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 24s 24s/step - loss: 177.3801 - accuracy: 5.7121e-05 - val_loss: 254.0947 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 23s 23s/step - loss: 177.3801 - accuracy: 5.7121e-05 - val_loss: 254.0947 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 34s 34s/step - loss: 177.3801 - accuracy: 5.7121e-05 - val_loss: 254.0947 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 26s 26s/step - loss: 177.3801 - accuracy: 5.7121e-05 - val_loss: 254.0947 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 23s 23s/step - loss: 177.3801 - accuracy: 5.7121e-05 - val_loss: 254.0947 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 23s 23s/step - loss: 177.3801 - accuracy: 5.7121e-05 - val_loss: 254.0947 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 28s 28s/step - loss: 177.3801 - accuracy: 5.7121e-05 - val_loss: 254.0947 - val_accuracy: 0.0000e+00\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "భంగం 72 హవా రకాల 9జి ఉడా ఈస్టర్న్ తిరిగి టోర్నమెంటు పాఠశాలలు నైరుతి మొదలయిన విటునికి విద్యా 930 తిరిగి దేశపు దేశపు దేశపు ప్రోఫైల్స్ఎకనమిక్ ప్రోఫైల్స్ఎకనమిక్ జిబ్రాల్టర్ జిబ్రాల్టర్ జిబ్రాల్టర్ జిబ్రాల్టర్ జిబ్రాల్టర్ జిబ్రాల్టర్ జిబ్రాల్టర్ జిబ్రాల్టర్ జిబ్రాల్టర్ జిబ్రాల్టర్ జిబ్రాల్టర్ జిబ్రాల్టర్ జిబ్రాల్టర్ జిబ్రాల్టర్ జిబ్రాల్టర్ జిబ్రాల్టర్ జిబ్రాల్టర్ జిబ్రాల్టర్ జిబ్రాల్టర్ జిబ్రాల్టర్ జిబ్రాల్టర్ జిబ్రాల్టర్ జిబ్రాల్టర్ జిబ్రాల్టర్ జిబ్రాల్టర్ జిబ్రాల్టర్ జిబ్రాల్టర్ జిబ్రాల్టర్ జిబ్రాల్టర్ జిబ్రాల్టర్ జిబ్రాల్టర్\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "def embed_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    learning_rate = 1e-3\n",
    "    rnn = GRU(64, return_sequences=True, activation=\"tanh\")\n",
    "    \n",
    "    embedding = Embedding(french_vocab_size, 64, input_length=input_shape[1]) \n",
    "    logits = TimeDistributed(Dense(french_vocab_size, activation=\"softmax\"))\n",
    "    \n",
    "    model = Sequential()\n",
    "    #em can only be used in first layer --> Keras Documentation\n",
    "    model.add(embedding)\n",
    "    model.add(rnn)\n",
    "    model.add(logits)\n",
    "    model.compile(loss=huber_loss,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "tmp_x = pad(preproc_english_sentences, max_french_sequence_length)\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2]))\n",
    "embeded_model = embed_model(\n",
    "    tmp_x.shape,\n",
    "    max_french_sequence_length,\n",
    "    english_vocab_size,\n",
    "    french_vocab_size)\n",
    "embeded_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)\n",
    "print(logits_to_text(embeded_model.predict(tmp_x[:1])[0], french_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4a414e14",
   "metadata": {
    "id": "4a414e14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1/1 [==============================] - 26s 26s/step - loss: 177.3801 - accuracy: 0.4931 - val_loss: 254.0947 - val_accuracy: 0.5052\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 22s 22s/step - loss: 177.3801 - accuracy: 0.4934 - val_loss: 254.0947 - val_accuracy: 0.5052\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 19s 19s/step - loss: 177.3801 - accuracy: 0.4956 - val_loss: 254.0947 - val_accuracy: 0.5052\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 22s 22s/step - loss: 177.3801 - accuracy: 0.4944 - val_loss: 254.0947 - val_accuracy: 0.5052\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 23s 23s/step - loss: 177.3801 - accuracy: 0.4929 - val_loss: 254.0947 - val_accuracy: 0.5052\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 20s 20s/step - loss: 177.3801 - accuracy: 0.4958 - val_loss: 254.0947 - val_accuracy: 0.5052\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 20s 20s/step - loss: 177.3801 - accuracy: 0.4949 - val_loss: 254.0947 - val_accuracy: 0.5052\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 22s 22s/step - loss: 177.3801 - accuracy: 0.4939 - val_loss: 254.0947 - val_accuracy: 0.5052\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 25s 25s/step - loss: 177.3801 - accuracy: 0.4936 - val_loss: 254.0947 - val_accuracy: 0.5052\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 20s 20s/step - loss: 177.3801 - accuracy: 0.4957 - val_loss: 254.0947 - val_accuracy: 0.5052\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - 21s 21s/step - loss: 177.3801 - accuracy: 0.4982 - val_loss: 254.0947 - val_accuracy: 0.5052\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - 22s 22s/step - loss: 177.3801 - accuracy: 0.4945 - val_loss: 254.0947 - val_accuracy: 0.5052\n",
      "Epoch 13/20\n",
      "1/1 [==============================] - 20s 20s/step - loss: 177.3801 - accuracy: 0.4949 - val_loss: 254.0947 - val_accuracy: 0.5052\n",
      "Epoch 14/20\n",
      "1/1 [==============================] - 23s 23s/step - loss: 177.3801 - accuracy: 0.4966 - val_loss: 254.0947 - val_accuracy: 0.5052\n",
      "Epoch 15/20\n",
      "1/1 [==============================] - 23s 23s/step - loss: 177.3801 - accuracy: 0.4937 - val_loss: 254.0947 - val_accuracy: 0.5052\n",
      "Epoch 16/20\n",
      "1/1 [==============================] - 23s 23s/step - loss: 177.3801 - accuracy: 0.4941 - val_loss: 254.0947 - val_accuracy: 0.5052\n",
      "Epoch 17/20\n",
      "1/1 [==============================] - 22s 22s/step - loss: 177.3801 - accuracy: 0.4992 - val_loss: 254.0947 - val_accuracy: 0.5052\n",
      "Epoch 18/20\n",
      "1/1 [==============================] - 27s 27s/step - loss: 177.3801 - accuracy: 0.4931 - val_loss: 254.0947 - val_accuracy: 0.5052\n",
      "Epoch 19/20\n",
      "1/1 [==============================] - 21s 21s/step - loss: 177.3801 - accuracy: 0.4925 - val_loss: 254.0947 - val_accuracy: 0.5052\n",
      "Epoch 20/20\n",
      "1/1 [==============================] - 20s 20s/step - loss: 177.3801 - accuracy: 0.4923 - val_loss: 254.0947 - val_accuracy: 0.5052\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000014271C3ECA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "పురుషార్థాలు గ్రామాలు రికార్డ్ రికార్డ్ పురుషార్థాలు రికార్డ్ రికార్డ్ గ్రామాలు గ్రామాలు రికార్డ్ గ్రామాలు గ్రామాలు గ్రామాలు కర్నూలు గ్రామాలు గ్రామాలు శుద్ధి శుద్ధి శుద్ధి శుద్ధి శుద్ధి శుద్ధి శుద్ధి శుద్ధి రాంనగర్ రాంనగర్ రాంనగర్ అనేక అనేక అనేక అనేక అనేక అనేక అనేక అనేక అనేక అనేక అనేక అనేక అనేక అనేక అనేక అనేక అనేక అనేక అనేక అనేక అనేక కాలంలో సి విశాఖపట్నం <PAD>\n"
     ]
    }
   ],
   "source": [
    "def bd_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "   \n",
    "    learning_rate = 1e-3\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(GRU(128, return_sequences = True, dropout = 0.1), \n",
    "                           input_shape = input_shape[1:]))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size, activation = 'softmax')))\n",
    "    model.compile(loss = huber_loss, \n",
    "                 optimizer = Adam(learning_rate), \n",
    "                 metrics = ['accuracy'])\n",
    "    return model\n",
    "tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\n",
    "bidi_model = bd_model(\n",
    "    tmp_x.shape,\n",
    "    preproc_french_sentences.shape[1],\n",
    "    len(english_tokenizer.word_index)+1,\n",
    "    len(french_tokenizer.word_index)+1)\n",
    "bidi_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=20, validation_split=0.2)\n",
    "# Print prediction(s)\n",
    "print(logits_to_text(bidi_model.predict(tmp_x[:1])[0], french_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2d1cc051",
   "metadata": {
    "id": "2d1cc051"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1/1 [==============================] - 28s 28s/step - loss: 177.3801 - accuracy: 0.8286 - val_loss: 254.0947 - val_accuracy: 0.8329\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 19s 19s/step - loss: 177.3801 - accuracy: 0.8286 - val_loss: 254.0947 - val_accuracy: 0.8329\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 18s 18s/step - loss: 177.3801 - accuracy: 0.8286 - val_loss: 254.0947 - val_accuracy: 0.8329\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 18s 18s/step - loss: 177.3801 - accuracy: 0.8286 - val_loss: 254.0947 - val_accuracy: 0.8329\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 17s 17s/step - loss: 177.3801 - accuracy: 0.8286 - val_loss: 254.0947 - val_accuracy: 0.8329\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 17s 17s/step - loss: 177.3801 - accuracy: 0.8286 - val_loss: 254.0947 - val_accuracy: 0.8329\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 17s 17s/step - loss: 177.3801 - accuracy: 0.8286 - val_loss: 254.0947 - val_accuracy: 0.8329\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 16s 16s/step - loss: 177.3801 - accuracy: 0.8286 - val_loss: 254.0947 - val_accuracy: 0.8329\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 18s 18s/step - loss: 177.3801 - accuracy: 0.8286 - val_loss: 254.0947 - val_accuracy: 0.8329\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 14s 14s/step - loss: 177.3801 - accuracy: 0.8286 - val_loss: 254.0947 - val_accuracy: 0.8329\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - 17s 17s/step - loss: 177.3801 - accuracy: 0.8286 - val_loss: 254.0947 - val_accuracy: 0.8329\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - 17s 17s/step - loss: 177.3801 - accuracy: 0.8286 - val_loss: 254.0947 - val_accuracy: 0.8329\n",
      "Epoch 13/20\n",
      "1/1 [==============================] - 17s 17s/step - loss: 177.3801 - accuracy: 0.8286 - val_loss: 254.0947 - val_accuracy: 0.8329\n",
      "Epoch 14/20\n",
      "1/1 [==============================] - 26s 26s/step - loss: 177.3801 - accuracy: 0.8286 - val_loss: 254.0947 - val_accuracy: 0.8329\n",
      "Epoch 15/20\n",
      "1/1 [==============================] - 16s 16s/step - loss: 177.3801 - accuracy: 0.8286 - val_loss: 254.0947 - val_accuracy: 0.8329\n",
      "Epoch 16/20\n",
      "1/1 [==============================] - 12s 12s/step - loss: 177.3801 - accuracy: 0.8286 - val_loss: 254.0947 - val_accuracy: 0.8329\n",
      "Epoch 17/20\n",
      "1/1 [==============================] - 18s 18s/step - loss: 177.3801 - accuracy: 0.8286 - val_loss: 254.0947 - val_accuracy: 0.8329\n",
      "Epoch 18/20\n",
      "1/1 [==============================] - 15s 15s/step - loss: 177.3801 - accuracy: 0.8286 - val_loss: 254.0947 - val_accuracy: 0.8329\n",
      "Epoch 19/20\n",
      "1/1 [==============================] - 16s 16s/step - loss: 177.3801 - accuracy: 0.8286 - val_loss: 254.0947 - val_accuracy: 0.8329\n",
      "Epoch 20/20\n",
      "1/1 [==============================] - 12s 12s/step - loss: 177.3801 - accuracy: 0.8286 - val_loss: 254.0947 - val_accuracy: 0.8329\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001426CD6FDC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "def encdec_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "  \n",
    "    learning_rate = 1e-3\n",
    "    model = Sequential()\n",
    "    model.add(GRU(128, input_shape = input_shape[1:], return_sequences = False))\n",
    "    model.add(RepeatVector(output_sequence_length))\n",
    "    model.add(GRU(128, return_sequences = True))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size, activation = 'softmax')))\n",
    "    \n",
    "    model.compile(loss = huber_loss, \n",
    "                 optimizer = Adam(learning_rate), \n",
    "                 metrics = ['accuracy'])\n",
    "    return model\n",
    "tmp_x = pad(preproc_english_sentences)\n",
    "tmp_x = tmp_x.reshape((-1, preproc_english_sentences.shape[1], 1))\n",
    "encodeco_model = encdec_model(\n",
    "    tmp_x.shape,\n",
    "    preproc_french_sentences.shape[1],\n",
    "    len(english_tokenizer.word_index)+1,\n",
    "    len(french_tokenizer.word_index)+1)\n",
    "encodeco_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=20, validation_split=0.2)\n",
    "print(logits_to_text(encodeco_model.predict(tmp_x[:1])[0], french_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d257dbd4",
   "metadata": {
    "id": "d257dbd4",
    "outputId": "53f81117-d58e-4d91-b64f-68d485e01781"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model Loaded\n"
     ]
    }
   ],
   "source": [
    "def model_final(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "  \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=english_vocab_size,output_dim=128,input_length=input_shape[1]))\n",
    "    model.add(Bidirectional(GRU(256,return_sequences=False)))\n",
    "    model.add(RepeatVector(output_sequence_length))\n",
    "    model.add(Bidirectional(GRU(256,return_sequences=True)))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size,activation='softmax')))\n",
    "    learning_rate = 0.005\n",
    "    \n",
    "    model.compile(loss = sparse_categorical_crossentropy, \n",
    "                 optimizer = Adam(learning_rate), \n",
    "                 metrics = ['accuracy'])\n",
    "    \n",
    "    return model\n",
    "print('Final Model Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78bdf123",
   "metadata": {
    "id": "78bdf123"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/17\n",
      "1/1 [==============================] - 79s 79s/step - loss: 8.4651 - accuracy: 3.8081e-05 - val_loss: 8.1150 - val_accuracy: 0.8608\n",
      "Epoch 2/17\n",
      "1/1 [==============================] - 58s 58s/step - loss: 8.1185 - accuracy: 0.8546 - val_loss: 3.5907 - val_accuracy: 0.8608\n",
      "Epoch 3/17\n",
      "1/1 [==============================] - 53s 53s/step - loss: 3.6123 - accuracy: 0.8546 - val_loss: 1.7595 - val_accuracy: 0.8608\n",
      "Epoch 4/17\n",
      "1/1 [==============================] - 47s 47s/step - loss: 1.8339 - accuracy: 0.8546 - val_loss: 1.7806 - val_accuracy: 0.8608\n",
      "Epoch 5/17\n",
      "1/1 [==============================] - 57s 57s/step - loss: 1.8175 - accuracy: 0.8546 - val_loss: 1.5394 - val_accuracy: 0.8608\n",
      "Epoch 6/17\n",
      "1/1 [==============================] - 52s 52s/step - loss: 1.5212 - accuracy: 0.8546 - val_loss: 1.7917 - val_accuracy: 0.8608\n",
      "Epoch 7/17\n",
      "1/1 [==============================] - 42s 42s/step - loss: 1.7259 - accuracy: 0.8546 - val_loss: 1.4864 - val_accuracy: 0.8608\n",
      "Epoch 8/17\n",
      "1/1 [==============================] - 38s 38s/step - loss: 1.3980 - accuracy: 0.8546 - val_loss: 1.5753 - val_accuracy: 0.8608\n",
      "Epoch 9/17\n",
      "1/1 [==============================] - 38s 38s/step - loss: 1.4674 - accuracy: 0.8546 - val_loss: 1.6376 - val_accuracy: 0.8608\n",
      "Epoch 10/17\n",
      "1/1 [==============================] - 39s 39s/step - loss: 1.5057 - accuracy: 0.8546 - val_loss: 1.6291 - val_accuracy: 0.8608\n",
      "Epoch 11/17\n",
      "1/1 [==============================] - 38s 38s/step - loss: 1.4591 - accuracy: 0.8546 - val_loss: 1.6271 - val_accuracy: 0.8608\n",
      "Epoch 12/17\n",
      "1/1 [==============================] - 37s 37s/step - loss: 1.4173 - accuracy: 0.8546 - val_loss: 1.6919 - val_accuracy: 0.8608\n",
      "Epoch 13/17\n",
      "1/1 [==============================] - 36s 36s/step - loss: 1.4547 - accuracy: 0.8546 - val_loss: 1.6881 - val_accuracy: 0.8608\n",
      "Epoch 14/17\n",
      "1/1 [==============================] - 40s 40s/step - loss: 1.4401 - accuracy: 0.8546 - val_loss: 1.6411 - val_accuracy: 0.8608\n",
      "Epoch 15/17\n",
      "1/1 [==============================] - 37s 37s/step - loss: 1.3943 - accuracy: 0.8546 - val_loss: 1.6390 - val_accuracy: 0.8608\n",
      "Epoch 16/17\n",
      "1/1 [==============================] - 43s 43s/step - loss: 1.4004 - accuracy: 0.8546 - val_loss: 1.6435 - val_accuracy: 0.8608\n",
      "Epoch 17/17\n",
      "1/1 [==============================] - 34s 34s/step - loss: 1.4070 - accuracy: 0.8546 - val_loss: 1.6342 - val_accuracy: 0.8608\n"
     ]
    }
   ],
   "source": [
    "def final_predictions(x, y, x_tk, y_tk):\n",
    "    global predictions\n",
    "    tmp_X = pad(preproc_english_sentences)\n",
    "    model = model_final(tmp_X.shape,\n",
    "                        preproc_french_sentences.shape[1],\n",
    "                        len(english_tokenizer.word_index)+1,\n",
    "                        len(french_tokenizer.word_index)+1)\n",
    "    \n",
    "    model.fit(tmp_X, preproc_french_sentences, batch_size = 1024, epochs = 17, validation_split = 0.2)\n",
    " \n",
    "    \n",
    "final_predictions(preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97927a48",
   "metadata": {
    "id": "97927a48"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a67bc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
