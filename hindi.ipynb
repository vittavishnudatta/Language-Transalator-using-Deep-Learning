{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e65fa4fd",
   "metadata": {
    "id": "e65fa4fd"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from pickle import dump\n",
    "from unicodedata import normalize\n",
    "from numpy import array\n",
    "import string\n",
    "import re\n",
    "from numpy import array, argmax, random, take\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, RepeatVector\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from keras import optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "from pickle import load\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.layers import TimeDistributed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "m034xz0yesRZ",
   "metadata": {
    "id": "m034xz0yesRZ"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Model\n",
    "from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy, huber_loss, categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ieUwiVWajC7w",
   "metadata": {
    "id": "ieUwiVWajC7w"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    Load Dataset from File\n",
    "    \"\"\"\n",
    "    input_file = os.path.join(path)\n",
    "    with open(input_file, \"r\",encoding=\"utf8\") as f:\n",
    "        data = f.read().splitlines()\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "817a3d30",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "817a3d30",
    "outputId": "a91fdd76-bb5b-4bc0-b105-6f0f3236b48d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded\n"
     ]
    }
   ],
   "source": [
    "english_sentences = load_data('hin_eng.txt')\n",
    "tamil_sentences = load_data('hin_hin.txt')\n",
    "print('Dataset Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "QUJWoZerkVM-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QUJWoZerkVM-",
    "outputId": "2d84801b-4446-4d82-ac86-d53b75b45272"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(english_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "064ef733",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "064ef733",
    "outputId": "1582fe4c-4cfb-49c4-a06c-8726289ea579"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small_vocab_en Line 1:  It is believed that before the formation of modern Delhi, Delhi was ruined seven times and established in different areas, some of the remains can be seen today also.\n",
      "small_vocab_fr Line 1:  ऐसा माना जाता है कि आज की आधुनिक दिल्ली बनने से पहले दिल्ली सात बार उजड़ी और विभिन्न स्थानों पर बसी जिनके कुछ अवशेष अब भी देखे जा सकते हैं।\n",
      "small_vocab_en Line 2:  Lalit Narayan Mithila University Darbhanga\n",
      "small_vocab_fr Line 2:  ललित नारायण मिथिला विश्वविद्यालय दरभंगा\n"
     ]
    }
   ],
   "source": [
    "for sample_i in range(2):\n",
    "    print('small_vocab_en Line {}:  {}'.format(sample_i + 1, english_sentences[sample_i]))\n",
    "    print('small_vocab_fr Line {}:  {}'.format(sample_i + 1, tamil_sentences[sample_i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "807fe3c4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "807fe3c4",
    "outputId": "59032ae0-2e48-4d2f-cbfa-2f7d372ed7f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13162 English words.\n",
      "4816 unique English words.\n",
      "10 Most common words in the English dataset:\n",
      "\"the\" \"of\" \"and\" \"in\" \"is\" \"to\" \"was\" \"Delhi\" \"a\" \"are\"\n",
      "\n",
      "14352 tamil words.\n",
      "4192 unique tamil words.\n",
      "10 Most common words in the tamil dataset:\n",
      "\"के\" \"में\" \"का\" \"की\" \"से\" \"है।\" \"और\" \"दिल्ली\" \"को\" \"है\"\n"
     ]
    }
   ],
   "source": [
    "english_words_counter = collections.Counter([word for sentence in english_sentences for word in sentence.split()])\n",
    "tamil_words_counter = collections.Counter([word for sentence in tamil_sentences for word in sentence.split()])\n",
    "print('{} English words.'.format(len([word for sentence in english_sentences for word in sentence.split()])))\n",
    "print('{} unique English words.'.format(len(english_words_counter)))\n",
    "print('10 Most common words in the English dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*english_words_counter.most_common(10)))[0]) + '\"')\n",
    "print()\n",
    "print('{} tamil words.'.format(len([word for sentence in tamil_sentences for word in sentence.split()])))\n",
    "print('{} unique tamil words.'.format(len(tamil_words_counter)))\n",
    "print('10 Most common words in the tamil dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*tamil_words_counter.most_common(10)))[0]) + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81799209",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "81799209",
    "outputId": "11a1cbaf-1f61-4e39-8f12-38559cd03c36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 1, 'quick': 2, 'a': 3, 'brown': 4, 'fox': 5, 'jumps': 6, 'over': 7, 'lazy': 8, 'dog': 9, 'by': 10, 'jove': 11, 'my': 12, 'study': 13, 'of': 14, 'lexicography': 15, 'won': 16, 'prize': 17, 'this': 18, 'is': 19, 'short': 20, 'sentence': 21}\n",
      "\n",
      "Sequence 1 in x\n",
      "  Input:  The quick brown fox jumps over the lazy dog .\n",
      "  Output: [1, 2, 4, 5, 6, 7, 1, 8, 9]\n",
      "Sequence 2 in x\n",
      "  Input:  By Jove , my quick study of lexicography won a prize .\n",
      "  Output: [10, 11, 12, 2, 13, 14, 15, 16, 3, 17]\n",
      "Sequence 3 in x\n",
      "  Input:  This is a short sentence .\n",
      "  Output: [18, 19, 3, 20, 21]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(x):\n",
    "    x_tk = Tokenizer(char_level = False)\n",
    "    x_tk.fit_on_texts(x)\n",
    "    return x_tk.texts_to_sequences(x), x_tk\n",
    "text_sentences = [\n",
    "    'The quick brown fox jumps over the lazy dog .',\n",
    "    'By Jove , my quick study of lexicography won a prize .',\n",
    "    'This is a short sentence .']\n",
    "text_tokenized, text_tokenizer = tokenize(text_sentences)\n",
    "print(text_tokenizer.word_index)\n",
    "print()\n",
    "for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(sent))\n",
    "    print('  Output: {}'.format(token_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0a61a93",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c0a61a93",
    "outputId": "28c40aa1-9ea9-4e01-910b-52afed4253fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1 in x\n",
      "  Input:  [1 2 4 5 6 7 1 8 9]\n",
      "  Output: [1 2 4 5 6 7 1 8 9 0]\n",
      "Sequence 2 in x\n",
      "  Input:  [10 11 12  2 13 14 15 16  3 17]\n",
      "  Output: [10 11 12  2 13 14 15 16  3 17]\n",
      "Sequence 3 in x\n",
      "  Input:  [18 19  3 20 21]\n",
      "  Output: [18 19  3 20 21  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "def pad(x, length=None):\n",
    "    if length is None:\n",
    "        length = max([len(sentence) for sentence in x])\n",
    "    return pad_sequences(x, maxlen = length, padding = 'post')\n",
    "# Pad Tokenized output\n",
    "test_pad = pad(text_tokenized)\n",
    "for sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(np.array(token_sent)))\n",
    "    print('  Output: {}'.format(pad_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c033886b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c033886b",
    "outputId": "eee95f2a-41eb-48b6-d9bf-fb8d0a5a798e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessed\n",
      "Max English sentence length: 130\n",
      "Max tamil sentence length: 208\n",
      "English vocabulary size: 3744\n",
      "Tamil vocabulary size: 4006\n"
     ]
    }
   ],
   "source": [
    "def preprocess(x, y):\n",
    "    preprocess_x, x_tk = tokenize(x)\n",
    "    preprocess_y, y_tk = tokenize(y)\n",
    "    preprocess_x = pad(preprocess_x)\n",
    "    preprocess_y = pad(preprocess_y)\n",
    "# Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
    "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
    "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
    "preproc_english_sentences, preproc_tamil_sentences, english_tokenizer, tamil_tokenizer =\\\n",
    "    preprocess(english_sentences, tamil_sentences)\n",
    "    \n",
    "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
    "max_tamil_sequence_length = preproc_tamil_sentences.shape[1]\n",
    "english_vocab_size = len(english_tokenizer.word_index)\n",
    "tamil_vocab_size = len(tamil_tokenizer.word_index)\n",
    "print('Data Preprocessed')\n",
    "print(\"Max English sentence length:\", max_english_sequence_length)\n",
    "print(\"Max tamil sentence length:\", max_tamil_sequence_length)\n",
    "print(\"English vocabulary size:\", english_vocab_size)\n",
    "print(\"Tamil vocabulary size:\", tamil_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9271a53",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b9271a53",
    "outputId": "5c0957b0-8748-42f1-a7e6-d61a44858a91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`logits_to_text` function loaded.\n"
     ]
    }
   ],
   "source": [
    "def logits_to_text(logits, tokenizer):\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<PAD>'\n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
    "print('`logits_to_text` function loaded.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5658710",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "f5658710",
    "outputId": "2143658f-ed86-49f6-d06f-a0d5b6bcb55f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7/7 [==============================] - 33s 4s/step - loss: 43.5886 - accuracy: 0.7761 - val_loss: 55.9497 - val_accuracy: 0.7778\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 28s 4s/step - loss: 43.5886 - accuracy: 0.7761 - val_loss: 55.9497 - val_accuracy: 0.7778\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 28s 4s/step - loss: 43.5886 - accuracy: 0.7762 - val_loss: 55.9497 - val_accuracy: 0.7778\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 28s 4s/step - loss: 43.5886 - accuracy: 0.7762 - val_loss: 55.9497 - val_accuracy: 0.7779\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 28s 4s/step - loss: 43.5886 - accuracy: 0.7762 - val_loss: 55.9497 - val_accuracy: 0.7780\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 28s 4s/step - loss: 43.5886 - accuracy: 0.7763 - val_loss: 55.9497 - val_accuracy: 0.7780\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 30s 4s/step - loss: 43.5886 - accuracy: 0.7762 - val_loss: 55.9497 - val_accuracy: 0.7779\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 31s 5s/step - loss: 43.5886 - accuracy: 0.7762 - val_loss: 55.9497 - val_accuracy: 0.7779\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 28s 4s/step - loss: 43.5886 - accuracy: 0.7761 - val_loss: 55.9497 - val_accuracy: 0.7777\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 27s 4s/step - loss: 43.5886 - accuracy: 0.7759 - val_loss: 55.9497 - val_accuracy: 0.7776\n",
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "4275",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 24>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m simple_rnn_model\u001b[38;5;241m.\u001b[39mfit(tmp_x, preproc_tamil_sentences, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Print prediction(s)\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mlogits_to_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43msimple_rnn_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmp_x\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtamil_tokenizer\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36mlogits_to_text\u001b[1;34m(logits, tokenizer)\u001b[0m\n\u001b[0;32m      2\u001b[0m index_to_words \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mid\u001b[39m: word \u001b[38;5;28;01mfor\u001b[39;00m word, \u001b[38;5;28mid\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mword_index\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m      3\u001b[0m index_to_words[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<PAD>\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([index_to_words[prediction] \u001b[38;5;28;01mfor\u001b[39;00m prediction \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39margmax(logits, \u001b[38;5;241m1\u001b[39m)])\n",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      2\u001b[0m index_to_words \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mid\u001b[39m: word \u001b[38;5;28;01mfor\u001b[39;00m word, \u001b[38;5;28mid\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mword_index\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m      3\u001b[0m index_to_words[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<PAD>\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[43mindex_to_words\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m prediction \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39margmax(logits, \u001b[38;5;241m1\u001b[39m)])\n",
      "\u001b[1;31mKeyError\u001b[0m: 4275"
     ]
    }
   ],
   "source": [
    "def simple_model(input_shape, output_sequence_length, english_vocab_size, tamil_vocab_size):\n",
    "    learning_rate = 1e-3\n",
    "    input_seq = Input(input_shape[1:])\n",
    "    rnn = GRU(64, return_sequences = True)(input_seq)\n",
    "    logits = TimeDistributed(Dense(4720))(rnn)\n",
    "    model = Model(input_seq, Activation('softmax')(logits))\n",
    "    model.compile(loss = huber_loss, \n",
    "                 optimizer = Adam(learning_rate), \n",
    "                 metrics = ['accuracy'])\n",
    "    \n",
    "    return model\n",
    "tmp_x = pad(preproc_english_sentences, max_tamil_sequence_length)\n",
    "tmp_x = tmp_x.reshape((-1, preproc_tamil_sentences.shape[-2], 1))\n",
    "tmp_x=np.delete(tmp_x,1,0)\n",
    "\n",
    "# Train the neural network\n",
    "simple_rnn_model = simple_model(\n",
    "    tmp_x.shape,\n",
    "    max_tamil_sequence_length,\n",
    "    english_vocab_size,\n",
    "    tamil_vocab_size)\n",
    "simple_rnn_model.fit(tmp_x, preproc_tamil_sentences, batch_size=128, epochs=10, validation_split=0.2)\n",
    "# Print prediction(s)\n",
    "print(logits_to_text(simple_rnn_model.predict(tmp_x[:1])[0], tamil_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c1f12a",
   "metadata": {
    "id": "60c1f12a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 376s 376s/step - loss: 43.5886 - accuracy: 2.7790e-05 - val_loss: 55.9497 - val_accuracy: 4.4311e-05\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 420s 420s/step - loss: 43.5886 - accuracy: 2.7790e-05 - val_loss: 55.9497 - val_accuracy: 4.4311e-05\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 371s 371s/step - loss: 43.5886 - accuracy: 2.7790e-05 - val_loss: 55.9497 - val_accuracy: 4.4311e-05\n",
      "Epoch 4/10\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "def embed_model(input_shape, output_sequence_length, english_vocab_size, tamil_vocab_size):\n",
    "    learning_rate = 1e-3\n",
    "    rnn = GRU(64, return_sequences=True, activation=\"tanh\")\n",
    "    \n",
    "    embedding = Embedding(tamil_vocab_size, 64, input_length=input_shape[1]) \n",
    "    logits = TimeDistributed(Dense(tamil_vocab_size, activation=\"softmax\"))\n",
    "    \n",
    "    model = Sequential()\n",
    "    #em can only be used in first layer --> Keras Documentation\n",
    "    model.add(embedding)\n",
    "    model.add(rnn)\n",
    "    model.add(logits)\n",
    "    model.compile(loss=huber_loss,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "tmp_x = pad(preproc_english_sentences, max_tamil_sequence_length)\n",
    "tmp_x = tmp_x.reshape((-1, preproc_tamil_sentences.shape[-2]))\n",
    "tmp_x=np.delete(tmp_x,1,0)\n",
    "\n",
    "embeded_model = embed_model(\n",
    "    tmp_x.shape,\n",
    "    max_tamil_sequence_length,\n",
    "    english_vocab_size,\n",
    "    tamil_vocab_size)\n",
    "embeded_model.fit(tmp_x, preproc_tamil_sentences, batch_size=1024, epochs=10, validation_split=0.2)\n",
    "print(logits_to_text(embeded_model.predict(tmp_x[:1])[0], tamil_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a414e14",
   "metadata": {
    "id": "4a414e14"
   },
   "outputs": [],
   "source": [
    "def bd_model(input_shape, output_sequence_length, english_vocab_size, tamil_vocab_size):\n",
    "   \n",
    "    learning_rate = 1e-3\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(GRU(128, return_sequences = True, dropout = 0.1), \n",
    "                           input_shape = input_shape[1:]))\n",
    "    model.add(TimeDistributed(Dense(tamil_vocab_size, activation = 'softmax')))\n",
    "    model.compile(loss = huber_loss, \n",
    "                 optimizer = Adam(learning_rate), \n",
    "                 metrics = ['accuracy'])\n",
    "    return model\n",
    "tmp_x = pad(preproc_english_sentences, preproc_tamil_sentences.shape[1])\n",
    "tmp_x = tmp_x.reshape((-1, preproc_tamil_sentences.shape[-2], 1))\n",
    "tmp_x=np.delete(tmp_x,1,0)\n",
    "\n",
    "bidi_model = bd_model(\n",
    "    tmp_x.shape,\n",
    "    preproc_tamil_sentences.shape[1],\n",
    "    len(english_tokenizer.word_index)+1,\n",
    "    len(tamil_tokenizer.word_index)+1)\n",
    "bidi_model.fit(tmp_x, preproc_tamil_sentences, batch_size=1024, epochs=20, validation_split=0.2)\n",
    "# Print prediction(s)\n",
    "print(logits_to_text(bidi_model.predict(tmp_x[:1])[0], tamil_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1cc051",
   "metadata": {
    "id": "2d1cc051"
   },
   "outputs": [],
   "source": [
    "def encdec_model(input_shape, output_sequence_length, english_vocab_size, tamil_vocab_size):\n",
    "  \n",
    "    learning_rate = 1e-3\n",
    "    model = Sequential()\n",
    "    model.add(GRU(128, input_shape = input_shape[1:], return_sequences = False))\n",
    "    model.add(RepeatVector(output_sequence_length))\n",
    "    model.add(GRU(128, return_sequences = True))\n",
    "    model.add(TimeDistributed(Dense(tamil_vocab_size, activation = 'softmax')))\n",
    "    \n",
    "    model.compile(loss = huber_loss, \n",
    "                 optimizer = Adam(learning_rate), \n",
    "                 metrics = ['accuracy'])\n",
    "    return model\n",
    "tmp_x = pad(preproc_english_sentences)\n",
    "tmp_x = tmp_x.reshape((-1, preproc_english_sentences.shape[1], 1))\n",
    "tmp_x=np.delete(tmp_x,1,0)\n",
    "\n",
    "encodeco_model = encdec_model(\n",
    "    tmp_x.shape,\n",
    "    preproc_tamil_sentences.shape[1],\n",
    "    len(english_tokenizer.word_index)+1,\n",
    "    len(tamil_tokenizer.word_index)+1)\n",
    "encodeco_model.fit(tmp_x, preproc_tamil_sentences, batch_size=1024, epochs=20, validation_split=0.2)\n",
    "print(logits_to_text(encodeco_model.predict(tmp_x[:1])[0], tamil_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d257dbd4",
   "metadata": {
    "id": "d257dbd4",
    "outputId": "53f81117-d58e-4d91-b64f-68d485e01781"
   },
   "outputs": [],
   "source": [
    "def model_final(input_shape, output_sequence_length, english_vocab_size, tamil_vocab_size):\n",
    "  \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=english_vocab_size,output_dim=128,input_length=input_shape[1]))\n",
    "    model.add(Bidirectional(GRU(256,return_sequences=False)))\n",
    "    model.add(RepeatVector(output_sequence_length))\n",
    "    model.add(Bidirectional(GRU(256,return_sequences=True)))\n",
    "    model.add(TimeDistributed(Dense(tamil_vocab_size,activation='softmax')))\n",
    "    learning_rate = 0.005\n",
    "    \n",
    "    model.compile(loss = sparse_categorical_crossentropy, \n",
    "                 optimizer = Adam(learning_rate), \n",
    "                 metrics = ['accuracy'])\n",
    "    \n",
    "    return model\n",
    "print('Final Model Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bdf123",
   "metadata": {
    "id": "78bdf123"
   },
   "outputs": [],
   "source": [
    "def final_predictions(x, y, x_tk, y_tk):\n",
    "    global predictions\n",
    "    tmp_X = pad(preproc_english_sentences)\n",
    "    model = model_final(tmp_X.shape,\n",
    "                        preproc_tamil_sentences.shape[1],\n",
    "                        len(english_tokenizer.word_index)+1,\n",
    "                        len(tamil_tokenizer.word_index)+1)\n",
    "    \n",
    "    model.fit(tmp_X, preproc_tamil_sentences, batch_size = 1024, epochs = 17, validation_split = 0.2)\n",
    " \n",
    "    \n",
    "final_predictions(preproc_english_sentences, preproc_tamil_sentences, english_tokenizer, tamil_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97927a48",
   "metadata": {
    "id": "97927a48"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a67bc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
